\section{Εκπαίδευση και αξιολόγηση}
Ένας αλγόριθμος μηχανικής εκμάθησης αποκτά εμπειρία σε ένα \newtermprint[dataset]{Σύνολο Δεδομένων}.
Τα δεδομένα που χρησιμοποιούνται για την εκπαίδευση του αλγορίθμου ονομάζονται \newterm{Σύνολο Εκπαίδευσης}{Training Set} και αξιοποιούνται για να προσαρμοστούν οι παράμετροι του μοντέλου.

Μια από τις σημαντικότερες προκλήσεις στον τομέα της μηχανικής μάθησης είναι η επιθυμία καλών επιδόσεων των μοντέλων ακόμα και σε δεδομένα πάνω στα οποία δεν έχουν εκπαιδευτεί.
Λέμε ότι ένα μοντέλο που μπορεί να αποδίδει καλά σε καινούργια δεδομένα έχει δυνατότητα \newterm{Γενίκευση\dd{ς}}{Generalization}.

Κατά τη διάρκεια της εκπαίδευσης, ο αλγόριθμος μηχανικής εκμάθησης προσπαθεί να ελαχιστοποιήσει κάποια μετρική που αξιολογεί το σφάλμα εξόδου, δηλαδή πραγματοποιεί κάποιο είδος βελτιστοποίησης.
Το σφάλμα που προκύπτει κατά την εκπαίδευση ονομάζεται \newterm{Σφάλμα Εκπαίδευσης}{Training Error}.
Αυτό που διαχωρίζει τη μηχανική εκμάθηση από την απλή βελτιστοποίηση είναι ότι γίνεται και προσπάθεια ελαχιστοποίησης του αναμενόμενου σφάλματος σε νέα δεδομένα που
λέγεται \newterm{Σφάλμα Γενίκευσης}{Generalization Error} ή \newtermsee{Σφάλμα Δοκιμής}{Test Error}{Σφάλμα Γενίκευσης - Generalization Error}~\cite{goodfellow}.

Ένα μοντέλο που έχει σημαντικά μεγαλύτερο σφάλμα δοκιμής από σφάλμα εκπαίδευσης λέμε ότι παρουσιάζει \newterm{Υπερπροσαρμογή}{Overfitting},
δηλαδή έχει φτωχή γενίκευση σε νέα δεδομένα.

Θα θεωρήσουμε ότι μια μηχανή μάθησης έχει δύο στάδια λειτουργίας, την \newtermprint[Training]{εκπαίδευση} και την \newtermprint[Inference]{επαγωγή}.

\section{Συναρτήσεις κόστους και απωλειών}
Αναφέρθηκε ότι καθώς ο αλγόριθμος προσπαθεί να αποτυπώσει τη συσχέτιση μεταξύ των εισόδων και των εξόδων στο σύνολο εκπαίδευσης, υπολογίζεται η απόδοσή του με τη χρήση κάποιας μετρικής.
Αυτή η μετρική συνήθως λέγεται \newterm{Συνάρτηση Κόστους}{Cost Function}.

Αρχικά, ορίζουμε ως \newterm{Συνάρτηση Απωλειών}{Loss Function} μια συνάρτηση που δέχεται ως είσοδο την πραγματική τιμή της εξόδου $y$ και μια πρόβλεψη $\hat{y}$ και υπολογίζει με κάποιο τρόπο κατά πόσο αυτές οι δύο τιμές διαφέρουν.
Μερικές συχνά χρησιμοποιούμενες συναρτήσεις απωλειών είναι η \newterm[Συνάρτηση Απωλειών - Loss Function]{Τετραγωνική Συνάρτηση Απωλειών}{Quadratic Loss Function}:
\begin{equation}
    \label{eq:quadratic-loss}
    L_{\text{quadratic}} (y, \hat{y}) = C (y - \hat{y})
\end{equation}
η \newterm[Συνάρτηση Απωλειών - Loss Function]{Λογιστική Συνάρτηση Απωλειών}{Logistic Loss Function}:
\begin{equation}
    \label{eq:logistic-loss}
    L_{\text{logistic}} (y, \hat{y}) = \log{(1 + e^{-y \hat{y}})}
\end{equation}
και η \newterm[Συνάρτηση Απωλειών - Loss Function]{Απώλεια Διεντροπίας}{Cross-Entropy Loss}:
\begin{equation}
    \label{eq:cross-entropy-loss}
    L_{\text{cross-entropy}} (y, \hat{y}) = -[y \log{(\hat{y})} + (1 - y)\log{(1 - \hat{y})}]
\end{equation}

Έτσι, μια συνάρτηση κόστους μπορεί να υπολογισθεί ως:
\begin{equation}
    J(\vec{\theta}) = \sum_i (L(h_{\vec{\theta}}(x_i), y_i))
\end{equation}
Όπου:
\begin{conditions}
    m                & ο αριθμός των δειγμάτων στο σύνολο εκπαίδευσης                                                                          \\
    x_i              & το $i$-οστό δείγμα εισόδου του συνόλου εκπαίδευσης                                                                      \\
    y_i              & το $i$-οστό δείγμα εξόδου του συνόλου εκπαίδευσης                                                                       \\
    \vth             & οι επιλεγμένοι παράμετροι του μοντέλου                                                                                  \\
    h_{\vec{\theta}} & η συνάρτηση του μοντέλου δεδομένων των παραμέτρων $\theta$\anoteleia{} αλλιώς λέγεται και \newterm{Υπόθεση}{Hypothesis}
\end{conditions}
Για παράδειγμα, μια συνάρτηση κόστους που χρησιμοποιείται συχνά για την παλινδρόμηση είναι το \newterm{Μέσο Τετραγωνικό Σφάλμα}{Mean Squared Error - MSE}:
\begin{equation}
    \label{eq:MSE}
    J_{\text{MSE}}(\vec{\theta}) = \frac{1}{m} \sum_i(h_{\vec{\theta}}(x_i) - y_i)^2
\end{equation}

Δεδομένης μιας συνάρτησης κόστους $J(\vth)$, μπορούμε να χρησιμοποιήσουμε μια μέθοδο βελτιστοποίησης για την επαναληπτική αναβάθμιση των παραμέτρων $\vth$ του μοντέλου.
Μια τέτοια μέθοδος είναι αυτή της
\newterm{Κατάβαση\dd{ς} Κλίσης}{Gradient Descent}
(ή \newtermsee{Μέγιστης Καθόδου}{Steepest Descent}{Κατάβαση Κλίσης - Gradient Descent})~\cite{boyd2004convex,rovithakis}:
\begin{equation}
    \vth \leftarrow \vth - \alpha \nabla_{\vth}{J(\vth)}
\end{equation}
όπου $a$ ο \newterm{Ρυθμός Μάθησης}{Learning Rate}.
Αναφέρεται και η \newterm{Στοχαστική Κάθοδος Κλίσης}{Stochastic Gradient Descent - SGD} όπου οι παράμετροι ενημερώνονται με κάθε δείγμα στο σύνολο εκπαίδευσης,
αντί να υπολογίζεται η κλίση $\nabla_{\vth}{J(\vth)}$ σε όλο το σύνολο εκπαίδευσης~\cite{goodfellow}.
Εναλλακτικά, μπορούμε να υπολογίζουμε την κλίση μόνο σε ένα υποσύνολο
(ονομάζεται \newterm{Μικρο-δέσμη}{Mini-batch})
σε κάθε ενημέρωση των παραμέτρων.

\section{Γραμμική παλινδρόμηση}\label{sec:linear-regression}
\ig{linear-regression}{\lcaption{Παράδειγμα γραμμικής παλινδρόμησης}{%
        Η γραμμική παλινδρόμηση προσαρμόζει το βάρος $w_1$ και την προδιάθεση $b$ έτσι ώστε η γραμμή $y = w_1 x + b$ να περνάει όσο το δυνατό πιο κοντά από τα σημεία του συνόλου εκπαίδευσης.
        Ο κώδικας για την παραγωγή του γραφήματος προσαρμόστηκε από
        \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html}.%
    }%
}
\ig[pos=t!]{log-and-linear-regression}{\lcaption{Παράδειγμα σύγκρισης καμπύλης λογιστικής και ευθείας γραμμικής παλινδρόμησης}{%
        Γραφήματα με δύο παραδείγματα κατανομών Bernoulli.
        Η γραμμική παλινδρόμηση μπορεί να χρησιμοποιηθεί ως ταξινομητής αν συγκρίνουμε την έξοδό του με το κατώφλι $0.5$,
        ταξινομώντας τις τιμές μεγαλύτερες από αυτό το όριο ως $1$ και τις υπόλοιπες ως $0$.

        Η ακρίβεια των μοντέλων αναφέρεται στο σύνολο εκπαίδευσης καθώς στο συγκεκριμένο παράδειγμα δεν μας ενδιαφέρει πώς γενικεύονται τα μοντέλα\anoteleia{}
        τα δεδομένα που χρησιμοποιήθηκαν δεν προέρχονται από κάποιο υπαρκτό πρόβλημα αλλά δημιουργήθηκαν για σκοπούς επίδειξης των μοντέλων.

        (\textit{Αριστερά})
        Τα δεδομένα είναι μια απλή γραμμή με γκαουσιανό θόρυβο.
        Ακρίβεια ταξινομητή λογιστικής παλινδρόμησης στο σύνολο εκπαίδευσης: $94\%$,
        ακρίβεια ταξινομητή γραμμικής παλινδρόμησης στο σύνολο εκπαίδευσης: $89\%$.
        Ο κώδικας για την παραγωγή του γραφήματος προσαρμόστηκε από το
        \url{https://github.com/scikit-learn/scikit-learn/blob/0.20.X/examples/linear_model/plot_logistic.py}.

        (\textit{Δεξιά})
        Σε αυτό το παράδειγμα φαίνεται πως μερικές έκτοπες τιμές μπορούν να επηρεάσουν το μοντέλο γραμμικής παλινδρόμησης.
        Ακρίβεια ταξινομητή λογιστικής παλινδρόμησης στο σύνολο εκπαίδευσης: $100\%$,
        ακρίβεια ταξινομητή γραμμικής παλινδρόμησης στο σύνολο εκπαίδευσης: $73.91\%$.%
    }%
}

Η \newterm{Γραμμική Παλινδρόμηση}{Linear Regression} αποτελεί έναν από τους απλούστερους αλγορίθμους μηχανικής εκμάθησης.
Κατασκευάζει μια συνάρτηση που δέχεται ένα διάνυσμα $\vx \in \mathbb{R}^n$ και προσεγγίζει μια βαθμωτή τιμή $y \in \mathbb{R}$.
Η συνάρτηση αυτή είναι της μορφής:
\begin{equation}
    \label{eq:linear-regression}
    f(\vx) = \vw^\intercal \vx + b
\end{equation}
Το διάνυσμα $\vw \in \mathbb{R}^n$ ονομάζεται \newtermprint[Weight Vector]{διάνυσμα βαρών} και καθορίζει πώς κάθε χαρακτηριστικό $x_i$ επηρεάζει την τελική προσέγγιση.
Ο όρος $b$ ονομάζεται \newterm{Πόλωση}{Bias} ή \newtermsee{Κατώφλι}{Threshold}{Πόλωση - Bias} και με αυτόν οι προβλέψεις του μοντέλου δεν χρειάζεται να διέρχονται απαραίτητα από την αρχή των αξόνων $(0, 0)$.
Μπορούμε να γράψουμε τη συνάρτηση απλώς ως $\hth(\vx) = \vth^\intercal \vx$ αν επαυξήσουμε το διάνυσμα $\vx$ με ένα επιπλέον στοιχείο σταθερό με $1$ και προσθέσουμε ακόμα ένα βάρος στην ίδια θέση του $\vw$
(από εδώ και στο εξής $\vth$, το διάνυσμα των παραμέτρων του μοντέλου)
που θα έχει τον ίδιο \enquote{ρόλο} με τον όρο της πόλωσης.

Μπορούμε να βρούμε τις καλύτερες παραμέτρους του μοντέλου αν ελαχιστοποιήσουμε το μέσο τετραγωνικό σφάλμα~\ref{eq:MSE} βρίσκοντας το σημείο που μηδενίζεται η κλίση του:
\begin{equation}
    \nabla_{\vth}(J_{\vth}) = 0
\end{equation}
Η λύση προκύπτει~\cite{goodfellow}:
\begin{equation}
    \vth = (\vX^\intercal \vX)^{-1} \vX \vec{y}
\end{equation}

\section{Λογιστική παλινδρόμηση}\label{sec:logistic-regression}
\ig{sigmoid}{\lcaption{Παράδειγμα σιγμοειδούς και Softmax}{%
        (\textit{Αριστερά})
        Η σιγμοειδής συνάρτηση στο διάστημα $[-20, 20]$.
        (\textit{Δεξιά})
        Η Softmax συνάρτηση όπου $\vec{z}$ ένα διάνυσμα 15 στοιχείων, ομοιόμορφα καταναμημένων στο διάστημα $[0, 25]$.%
    }%
}

Η \newtermprint{(διωνυμική)} \newterm{Λογιστική Παλινδρόμηση}{Logistic Regression} ή
\newtermsee{Logit Παλινδρόμηση}{}{Λογιστική Παλινδρόμηση}
προβλέπει την πιθανότητα του διανύσματος εισόδου $\vx$ να αντιστοιχεί σε μία από τις δύο κλάσεις της διχοτομικής εξόδου $y$.
Χρησιμοποιεί την έξοδο της γραμμικής συνάρτησης ως είσοδο στη
\newterm{Σιγμοειδή\rr{}{ς Συνάρτηση}}{Sigmoid\rr{}{ Function}} συνάρτηση (βλέπε και \fref{fig:sigmoid}):
\begin{equation}
    \label{eq:sigmoid}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}
Η σιγμοειδής συνάρτηση είναι χρήσιμη γιατί η έξοδός της είναι φραγμένη στο $0$ και $1$.
Παρουσιάζει κορεσμό όταν η είσοδός της είναι πολύ μεγάλη σε απόλυτη τιμή, δηλαδή η έξοδός της παρουσιάζει μικρή ευαισθησία σε μικρές μεταβολές της εισόδου.
Συνδυάζοντας την \fref{eq:linear-regression} με την \fref{eq:sigmoid} προκύπτει η συνάρτηση υπόθεσης:
\begin{equation}
    \label{eq:logistic-regression}
    \hth(\vx) = \frac{1}{1 + e^{-\vth \vx}}
\end{equation}

Στη λογιστική παλινδρόμηση δεν υπάρχει λύση κλειστής μορφής όπως στη γραμμική, αλλά πρέπει να ψάξουμε για τη λύση με μεθόδους βελτιστοποίησης~\cite{goodfellow}.

\subsection{Softmax παλινδρόμηση}
Η \newterm{Softmax Παλινδρόμηση}{Softmax Regression}
ή \newtermsee{Λογιστική Παλινδρόμηση Πολλών Κλάσεων}{Multiclass Logistic Regression}{Softmax}
είναι μια μέθοδος ταξινόμησης που γενικεύει τη λογιστική παλινδρόμηση σε προβλήματα πολλών κλάσεων~\cite{tao2015bearing}.
Βασίζεται στη χρήση της συνάρτησης Softmax
που δέχεται ένα διάνυσμα $z \in \mathbb{R}^k$ και το κανονικοποιεί σε μια κατανομή πιθανότητας\footnote{Μετά την εφαρμογή της Softmax, όλα τα στοιχεία βρίσκονται στο διάστημα $(0, 1)$ και το άθροισμά τους είναι $1$.}
(βλέπε και \fref{fig:sigmoid}):
\begin{equation}
    \label{eq:softmax}
    \text{softmax}(\vec{z}_i) = \frac{e^{z_i}}{\sum_j{e^{z_j}}}
\end{equation}
Έτσι, η συνάρτηση υπόθεσης παίρνει τη μορφή~\cite{tao2015bearing}:
\begin{equation}
    \label{eq:softmax-regression}
    \hTh(\vx_i)
    = \begin{bmatrix}
        p(y_i = 1 | \vx_i; \vTh) \\
        p(y_i = 2 | \vx_i; \vTh) \\
        \vdots                   \\
        p(y_i = k | \vx_i; \vTh)
    \end{bmatrix}
    = \frac{1}{\sum_{j=1}^k{{e^{\vth_j^\intercal \vx_i}}}} \begin{bmatrix}
        e^{\vth_1^\intercal \vx_i} \\
        e^{\vth_2^\intercal \vx_i} \\
        \vdots                     \\
        e^{\vth_k^\intercal \vx_i}
    \end{bmatrix}
\end{equation}
Όπου:
\begin{conditions}
    m    & ο αριθμός των δειγμάτων στο σύνολο εκπαίδευσης                       \\
    k    & ο αριθμός των κλάσεων                                                \\
    n    & η διάσταση του διανύσματος εισόδου $\vx$                             \\
    \vTh & ο $k \times (n + 1)$ πίνακας των επιλεγμένων παραμέτρων του μοντέλου
\end{conditions}

\subsection{Στρατηγική ενός εναντίον όλων}
Μια άλλη τεχνική για την εφαρμογή της λογιστικής παλινδρόμησης σε προβλήματα πολλών κλάσεων είναι η
\newterm{Στρατηγική ενός εναντίον όλων}{One-vs-all Strategy}
όπου ένας ταξινομητής λογιστικής παλινδρόμησης (ή και οποιοσδήποτε άλλος δυαδικός ταξινομητής)
χρησιμοποιείται για την πρόβλεψη της πιθανότητας κάθε κλάσης ξεχωριστά, δηλαδή σε αντίθεση με τις υπόλοιπες κλάσεις.

\section{Υπό συνθήκη τυχαίο πεδίο}\label{sec:crf}
\ig[type=tikz]{pgm}{\lcaption{Παράδειγμα πιθανοτικού γράφου}{%
        Κάθε βέλος υποδηλώνει πιθανοτική εξάρτηση.
        Το C εξαρτάται άμεσα από τα A και D, το D από τα A, B και C, το E από το C και το F από τα C και D\@.
        \\Σχέσεις ανεξαρτησίας που προκύπτουν από τον γράφο: \begin{tabular}{>{$}c<{$}}
            (A \perp B)                 \\
            (C \perp B \mid D)          \\
            (E \perp A, B, D, F \mid C) \\
            (F \perp A, B \mid C, D)
        \end{tabular}
        \\Παραγοντοποίηση:
        $ P(A, B, C, D, E, F) = P(A) P(B) P(C, D \mid A, B) P(E \mid C) P(F \mid C, D)$%
    }%
}

Ένα \newterm{Πιθανοτικό Γραφικό Μοντέλο}{Probabilistic Graphical Model - PGM} (βλέπε και \fref{fig:pgm})
μοντελοποιεί μια σύνθετη κατανομή πολλών τυχαίων μεταβλητών ως γινόμενο πολλών τοπικών συναρτήσεων που η καθεμία εξαρτάται από ένα μικρό πλήθος μεταβλητών.
Μας επιτρέπει να περιγράψουμε πώς μια παραγοντοποίηση της πιθανότητας ενός γεγονότος αντιστοιχεί σε ένα σύνολο υπό όρους σχέσεων ανεξαρτησίας που ικανοποιούν την κατανομή.
Διευκολύνει τη μοντελοποίηση πιο σύνθετων προβλημάτων καθώς αυτά συνήθως χαρακτηρίζονται από σημαντικό αριθμό εξαρτημένων και ανεξάρτητων γεγονότων~\cite{sutton2012introduction}.
Τα \newterm{Υπό Συνθήκη Τυχαίο Πεδί\rr{α}{ο}}{Conditional Random Field - CRF}~\cite{lafferty2001conditional} αποτελούν ένα τέτοιο μοντέλο.

Συνήθως χρησιμοποιούνται για την κατασκευή πιθανοτικών μοντέλων που τμηματίζουν ή ταξινομούν σειριακά δεδομένα, όπως κείμενα.
Η χρήση τους στον τομέα της επεξεργασίας φυσικής γλώσσας συνδέεται με την ικανότητά τους να πραγματοποιούν ταξινόμηση εξαρτώμενη από τα συμφραζόμενα.

Ορίζουμε ένα
\newterm[Υπό Συνθήκη Τυχαίο Πεδίο - Conditional Random Field - CRF]{\dd{υπό συνθήκη τυχαίο πεδίο }Γραμμικής Αλυσίδας}{Linear Chain\dd{ CRF}}%
\footnote{Από εδώ και στο εξής θα αναφέρονται ως \enquote{CRF γραμμικής αλυσίδας} ή απλώς \enquote{CRF}}
(\enquote{γραμμικής αλυσίδας} σε αντίθεση με τα \enquote{γενικευμένα} που θα αναφερθούν παρακάτω)
τη διανομή $p(\vy \mid \vx)$ που παίρνει τη μορφή:~\cite{lafferty2001conditional,sutton2012introduction}
\begin{equation}
    \label{eq:linear-crf}
    p(\vy \mid \vx) = \frac{1}{Z(\vx)} \prod_{t=1}^T{\exp\left[\sum_{j=1}^k{\theta_j f_j(y_t, y_{t-1}, \vx_t)}\right]}
\end{equation}
Όπου:
\begin{conditions}
    k        & ο αριθμός των κλάσεων                                            \\
    \vy      & το διάνυσμα εξόδου, μια ακολουθία ετικετών $T$ στοιχείων         \\
    y_t      & η $t$-οστή ετικέτα στο διάνυσμα εξόδου                           \\
    \vx      & η ακολουθία εισόδου, περιλαμβάνει $T$ διανύσματα χαρακτηριστικών \\
    \vx_t    & το $t$-οστό διάνυσμα χαρακτηριστικών                             \\
    \theta_j & το $j$-οστό στοιχείο του διανύσματος παραμέτρων (ή βαρών) $\vth$ \\
    f_j      & η $j$-οστή συνάρτηση χαρακτηριστικών                             \\
    Z(\vx)   & η συνάρτηση κανονικοποίησης που παίρνει τη μορφή:
    \begin{equation}
        Z(\vx) = \sum_{\vy} \prod_{t=1}^T{\exp{\left[\sum_{j=1}^k{\theta_j f_j(y_t, y_{t-1}, \vx_t)}\right]}}
    \end{equation}
\end{conditions}

\ig[pos=t,type=tikz]{linear-crf}{\lcaption{Διαγραμματική αναπαράσταση υπό συνθήκη τυχαίου πεδίου γραμμικής αλυσίδας}{%
        Μια ετικέτα $y_t$ επηρεάζεται μόνο από την προηγούμενη
        αλλά μπορεί να επηρεαστεί από όλα τα $\vx_t$.%
    }%
}

Σε ένα CRF γραμμικής αλυσίδας, βλέπουμε ότι οι συναρτήσεις χαρακτηριστικών εξαρτώνται μόνο από την τρέχουσα και την προηγούμενη ετικέτα.
Αυτός ο περιορισμός δεν ισχύει σε ένα \newterm[Υπό Συνθήκη Τυχαίο Πεδίο - Conditional Random Field - CRF]{Γενικευμένο\dd{ υπό συνθήκη τυχαίο πεδίο}}{General\dd{ CRF}}.
Αποτελεί ένα μη κατευθυνόμενο γραφικό μοντέλο όπου οι κόμβοι του διαιρούνται σε δύο ξένα σύνολα, $\vX$ και $\vY$, και μοντελοποιείται η διανομή $p\left(\vY \mid \vX\right)$.

Η λειτουργία αυτών των μοντέλων για την ταξινόμηση βασίζεται στη χρήση χαρακτηριστικών (\en{Features}) που εξάγονται από τα δεδομένα.
Μια \newterm{Συνάρτηση Χαρακτηριστικών}{Feature Function} $f_j$ εκφράζει κάποιο χαρακτηριστικό ενός στοιχείου της ακολουθίας εισόδου και η έξοδός της είναι δυαδική.
Αν το αντίστοιχο βάρος $\theta_j$ είναι μεγάλο, το χαρακτηριστικό που εκφράζει η συνάρτηση είναι σημαντικό και επηρεάζει πιο έντονα την επιλογή της ετικέτας $y_t$.
Μερικά παραδείγματα συναρτήσεων χαρακτηριστικών σε μια εφαρμογή \newterm{Επισημείωση\dd{ς} Μερών του Λόγου}{Part-of-Speech Tagging}:
\begin{compactitem}
    \item $f_1(y_t, y_{t-1}, \vx_t) = 1$ αν $y_{t-1}$ ουσιαστικό και $y_t$ ρήμα
    \item $f_2(y_t, y_{t-1}, \vx_t) = 1$ αν $y_t$ επίρρημα και η κατάληξη της $t$-οστής λέξης είναι \engquote{-ly}
    \item $f_3(y_t, y_{t-1}, \vx_t) = 1$ αν η $t$-οστή λέξη αρχίζει με κεφαλαίο και $y_{t-1}$ σημείο στίξης
\end{compactitem}

Σε αντίθεση με τα μοντέλα \hyperref[sec:linear-regression]{γραμμικής} και \hyperref[sec:logistic-regression]{λογιστικής} παλινδρόμησης,
τα CRF πραγματοποιούν ταξινόμηση εξαρτώμενη από τα συμφραζόμενα, δηλαδή μοντελοποιούν και τις σχέσεις ανάμεσα στα διανύσματα εισόδου.

\section{Νευρωνικά δίκτυα}\label{sec:neural-networks}
\ig[type=tikz]{neural-network}{\lcaption{Ένα νευρωνικό δίκτυο}{%
        Από \breakurl{http://www.texample.net/tikz/examples/neural-network/}.%
    }%
}
Τα \newterm{Νευρωνικά Δίκτυα}{Neural Networks} ή \newtermsee{Τεχνητά Νευρωνικά Δίκτυα}{Artificial Neural Networks - ANN}{Νευρωνικά Δίκτυα - Neural Networks}
αποτελούν ένα μαθηματικό μοντέλο που αρχικά εμπνεύσθηκε από τη λειτουργία του βιολογικού εγκεφάλου, χωρίς όμως να προσπαθεί να την προσομοιώσει~\cite{goodfellow}.

Βασικό δομικό στοιχείο ενός νευρωνικού δικτύου είναι ο \newterm[Νευρωνικά Δίκτυα - Neural Networks]{Νευρώνας}{Neuron},
συνάρτηση που λαμβάνει $M$ εισόδους και δίνει μία έξοδο:
\begin{equation}
    f(\vx) = \text{act}\left(\sum_i{w_i x_i} + b\right) = \text{act}\left(\vw^\intercal \vx + b\right)
\end{equation}
Όπου:
\begin{conditions}
    \vw        & \newtermprint[Weight Vector]{διάνυσμα βαρών}                                                      \\
    \vx        & το διάνυσμα εισόδου                                                                               \\
    b          & η \newterm[Νευρωνικά Δίκτυα - Neural Networks]{Πόλωση}{Bias}                                      \\
    \text{act} & η \newterm[Νευρωνικά Δίκτυα - Neural Networks]{Συνάρτηση Ενεργοποίησης}{Activation Function} που,
    αν δεν είναι γραμμική, μπορεί να επιτρέψει στο νευρωνικό δίκτυο να μοντελοποιήσει μη-γραμμικά προβλήματα.
    Μπορεί, για παράδειγμα, να είναι η σιγμοειδής συνάρτηση ή η \newterm[Νευρωνικά Δίκτυα - Neural Networks]{Μονάδα Γραμμικού Ανορθωτή}{Rectified Linear Unit - ReLU}:
    \begin{equation}
        f(x) = \max(0, x)
    \end{equation}
\end{conditions}
Με την παράλληλη τοποθέτηση $N$ νευρώνων δημιουργείται μια \newterm{Βαθμίδα}{Layer} που έχει έξοδο:
\begin{equation}
    \vy = \text{act}\left(\mat{W}^\intercal \vx + \vec{b}\right)
\end{equation}
Ένα νευρωνικό δίκτυο (βλέπε και \fref{fig:neural-network}) δημιουργείται με τη σύνθεση πολλών τέτοιων βαθμίδων.
Η πρώτη, που ονομάζεται \newtermprint[Input Layer]{Βαθμίδα Εισόδου}, δέχεται την είσοδο του μοντέλου
και η τελευταία, \newtermprint[Output Layer]{Βαθμίδα Εξόδου}, υπολογίζει την έξοδο.

Από την περίοδο της πρώτης εφαρμογής τους η μεθοδολογία χρήσης νευρωνικών δικτύων έχει εξελιχθεί σημαντικά.
Μερικές σημαντικές εξελίξεις αποτελούν~\cite{goodfellow}:
\begin{itemize}
    \item Η \newtermprint[Distributed Representation]{Διανεμημένη Αναπαράσταση}~\cite{hinton1986learning},
          σύμφωνα με την οποία κάθε νευρώνας συμμετέχει στην αναπαράσταση περισσότερων της μιας έννοιας και κάθε έννοια αναπαριστάται από περισσότερους του ενός νευρώνα.
    \item Ο αλγόριθμος της \newterm[Νευρωνικά Δίκτυα - Neural Networks]{Οπισθοδιάδοση\dd{ς}}{Backpropagation}~\cite{werbos1974beyond,lecun1985procedure,parker1985learning,rumelhart1988learning}
          ο οποίος επιτρέπει στην πληροφορία που προκύπτει από το σφάλμα στην μονάδα εξόδου να διαδοθεί στους νευρώνες όλου του δικτύου με σκοπό τον υπολογισμό της κλίσης για την ενημέρωσή τους.
    \item Τα \newterm[Νευρωνικά Δίκτυα - Neural Networks]{\dd{Δίκτυα }Μακράς Βραχυπρόθεσμης Μνήμης}{Long Short-Term Memory - LSTM} των \citet{hochreiter1997long}
          που αποτελούν έναν τύπο \newterm[Νευρωνικά Δίκτυα]{Ανατροφοδοτούμεν\rr{ων}{α}\dd{ νευρωνικών δικτύων}}{Recurrent\dd{ Neural Networks} - RNN}.

          Τα RNN είναι ένας τύπος νευρωνικού δικτύου που πέρα από την είσοδο $\vx$ αξιοποιούν και πληροφορίες από προηγούμενες εκτελέσεις τους.
          Αυτό τους επιτρέπει να παρουσιάζουν δυναμική συμπεριφορά για μια ακολουθία.
          Στα RNN συναντάται το πρόβλημα της \newtermprint[Vanishing Gradient]{εξαφανιζόμενης κλίσης} κατά το οποίο οι κλίσεις των πρώτων κρυφών βαθμίδων αποκτούν πολύ μικρές τιμές με αποτέλεσμα την πολύ αργή ανανέωση των βαρών τους.
          Τα LSTM δίκτυα επιχειρούν να αντιμετωπίσουν αυτό το πρόβλημα με τα κελιά μακράς βραχυπρόθεσμης μνήμης που έχουν τη δυνατότητα να αποθηκεύουν μια πληροφορία για μεγάλα χρονικά διαστήματα.

          Τα LSTM χρησιμοποιούνται ευρέως σήμερα για τη μοντελοποίηση ακολουθιών και βρίσκουν εφαρμογή στην επεξεργασία φυσικής γλώσσας.
    \item Η επικράτηση μοντέλων \newterm[Νευρωνικά Δίκτυα - Neural Networks]{Βαθιά\dd{ς} Μάθηση\dd{ς}}{Deep Learning}
          όπως τα \newterm[Νευρωνικά Δίκτυα]{Βαθιά Δίκτυα Πεποιθήσεων}{Deep Belief Networks - DBN}~\cite{hinton2006fast},
          δηλαδή, νευρωνικών δικτύων που βασίζονται στην ύπαρξη πολλών κρυφών βαθμίδων ανάμεσα στην είσοδο και την έξοδο.
\end{itemize}

\ig[type=tikz]{LSTM}{\ccaption{Ένα κελί LSTM}{sennhauser2018evaluating}}

% vim:ts=4:sw=4:expandtab:fo-=tc:tw=120
